{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorly as tl\n",
    "import tensorly.decomposition as tldec \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = np.random.random((10, 10, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.82207329, 0.9420222 , 0.27899614, 0.79602922, 0.98895746,\n",
       "         0.562426  , 0.50371534, 0.84261989, 0.85836972, 0.28953628],\n",
       "        [0.74579835, 0.32568329, 0.12986719, 0.98793715, 0.49842745,\n",
       "         0.08300429, 0.10671885, 0.02543785, 0.41774241, 0.86382963],\n",
       "        [0.74461656, 0.28881629, 0.25449231, 0.8542046 , 0.1624885 ,\n",
       "         0.66368394, 0.57455713, 0.99727968, 0.44272303, 0.17041027],\n",
       "        [0.98139399, 0.90907539, 0.93785202, 0.37127623, 0.08534596,\n",
       "         0.71371593, 0.85760864, 0.63733075, 0.79489343, 0.65324378],\n",
       "        [0.4092391 , 0.07387796, 0.47692241, 0.78028155, 0.16623292,\n",
       "         0.44481688, 0.48066373, 0.19053311, 0.73895736, 0.16441765],\n",
       "        [0.1330739 , 0.3957939 , 0.68110818, 0.69564948, 0.91199384,\n",
       "         0.75916287, 0.17010984, 0.19735753, 0.09769318, 0.35308816],\n",
       "        [0.87296452, 0.16286193, 0.36797862, 0.66689567, 0.63677524,\n",
       "         0.62862899, 0.02004647, 0.09652285, 0.38015941, 0.3117757 ],\n",
       "        [0.55271423, 0.448064  , 0.90400773, 0.82373961, 0.70815564,\n",
       "         0.17090819, 0.24074234, 0.62136189, 0.15574968, 0.00238464],\n",
       "        [0.90475839, 0.45352844, 0.20057055, 0.58484921, 0.57188887,\n",
       "         0.87192628, 0.77816145, 0.27401301, 0.88849511, 0.29014133],\n",
       "        [0.89724153, 0.07338235, 0.07983638, 0.28430597, 0.15103417,\n",
       "         0.90599898, 0.50733081, 0.87419792, 0.324647  , 0.88027568]],\n",
       "\n",
       "       [[0.30095884, 0.63110707, 0.44560436, 0.39685486, 0.12172631,\n",
       "         0.78274067, 0.7444959 , 0.91910809, 0.9508713 , 0.01948751],\n",
       "        [0.03896452, 0.33919947, 0.1060709 , 0.6773417 , 0.0588346 ,\n",
       "         0.69248374, 0.58860621, 0.70554269, 0.89867455, 0.45777615],\n",
       "        [0.96699309, 0.88555484, 0.02357454, 0.88052738, 0.21352747,\n",
       "         0.41297947, 0.24460555, 0.26748742, 0.33544346, 0.33913686],\n",
       "        [0.09287441, 0.04338717, 0.52852206, 0.4745111 , 0.22857903,\n",
       "         0.61548602, 0.08791181, 0.66993085, 0.6371494 , 0.43267428],\n",
       "        [0.62770721, 0.73481322, 0.88628625, 0.70750588, 0.51663953,\n",
       "         0.76976998, 0.19615302, 0.82704183, 0.90181718, 0.30398776],\n",
       "        [0.16315636, 0.58844811, 0.86887758, 0.71286911, 0.62476593,\n",
       "         0.79979862, 0.58652932, 0.61573315, 0.14076805, 0.99328804],\n",
       "        [0.12334733, 0.60341563, 0.27269854, 0.19496924, 0.80397709,\n",
       "         0.31688582, 0.36495991, 0.22633888, 0.27405222, 0.8316599 ],\n",
       "        [0.18944992, 0.05442828, 0.09694831, 0.91297503, 0.76047918,\n",
       "         0.22937225, 0.59520494, 0.24188871, 0.92602909, 0.92075976],\n",
       "        [0.40900628, 0.96036558, 0.78011268, 0.64655698, 0.88553145,\n",
       "         0.6607435 , 0.09391678, 0.44084303, 0.21619717, 0.24777347],\n",
       "        [0.92775763, 0.1974669 , 0.43928895, 0.93727133, 0.4052587 ,\n",
       "         0.11340344, 0.85902541, 0.89968585, 0.87145903, 0.75291585]],\n",
       "\n",
       "       [[0.62782176, 0.01576382, 0.39600593, 0.34321495, 0.675214  ,\n",
       "         0.50099332, 0.62937009, 0.95464347, 0.44330701, 0.77144291],\n",
       "        [0.97663492, 0.59555259, 0.42864124, 0.26928413, 0.87812453,\n",
       "         0.44198758, 0.68434794, 0.61554208, 0.74377697, 0.02689192],\n",
       "        [0.95318809, 0.80657914, 0.53318203, 0.18227557, 0.75710636,\n",
       "         0.02469397, 0.20996263, 0.83029183, 0.94605209, 0.45800952],\n",
       "        [0.42978591, 0.07194544, 0.55221792, 0.00492665, 0.57774694,\n",
       "         0.55835802, 0.86855802, 0.45415427, 0.94893844, 0.36421004],\n",
       "        [0.92363136, 0.10581643, 0.25033517, 0.03214017, 0.52118427,\n",
       "         0.28348717, 0.79373882, 0.30584505, 0.5727099 , 0.89133656],\n",
       "        [0.0998164 , 0.51159451, 0.81079924, 0.09170179, 0.96466534,\n",
       "         0.053102  , 0.93946833, 0.48522716, 0.3963285 , 0.74044688],\n",
       "        [0.24047441, 0.89845092, 0.93594758, 0.946377  , 0.53334941,\n",
       "         0.76161524, 0.54936903, 0.27841592, 0.25202873, 0.15208455],\n",
       "        [0.27884158, 0.51506229, 0.71759767, 0.19718684, 0.57739179,\n",
       "         0.4482113 , 0.02030251, 0.4884236 , 0.81675599, 0.82665296],\n",
       "        [0.21471195, 0.25306152, 0.00604039, 0.25140576, 0.61237112,\n",
       "         0.84843266, 0.97999803, 0.91735738, 0.01028017, 0.6929998 ],\n",
       "        [0.47078882, 0.49221705, 0.78888772, 0.65099245, 0.05688368,\n",
       "         0.47787411, 0.07573658, 0.69794001, 0.8630159 , 0.71644778]],\n",
       "\n",
       "       [[0.46600646, 0.2571127 , 0.32810449, 0.70810697, 0.17255441,\n",
       "         0.24963784, 0.63474033, 0.92967643, 0.05614549, 0.93649902],\n",
       "        [0.37799854, 0.60799734, 0.43309916, 0.51302119, 0.58521966,\n",
       "         0.91064237, 0.9229566 , 0.04047092, 0.43854978, 0.543129  ],\n",
       "        [0.60313128, 0.78128057, 0.77153417, 0.07203014, 0.61868277,\n",
       "         0.04196724, 0.34376196, 0.29821612, 0.65233736, 0.89637214],\n",
       "        [0.34966213, 0.14424436, 0.62902368, 0.87442787, 0.28563278,\n",
       "         0.53214139, 0.69396887, 0.7625701 , 0.15244174, 0.26313618],\n",
       "        [0.18049578, 0.78292635, 0.59401401, 0.24534419, 0.59439213,\n",
       "         0.06096058, 0.77720146, 0.20676849, 0.58829084, 0.84000361],\n",
       "        [0.12629643, 0.9778776 , 0.20227032, 0.02996263, 0.6762499 ,\n",
       "         0.92691441, 0.59699776, 0.29249885, 0.18588257, 0.87937259],\n",
       "        [0.04094936, 0.30721012, 0.98613799, 0.95679072, 0.8772748 ,\n",
       "         0.08026823, 0.89161887, 0.38342183, 0.38326022, 0.25568349],\n",
       "        [0.26957789, 0.21821735, 0.09321611, 0.2725592 , 0.95737547,\n",
       "         0.62145136, 0.07597979, 0.1149037 , 0.84007336, 0.05534625],\n",
       "        [0.99421205, 0.26746702, 0.26826699, 0.36912953, 0.51163327,\n",
       "         0.17454333, 0.86437506, 0.65524145, 0.1229073 , 0.62990063],\n",
       "        [0.73616481, 0.67065983, 0.29105411, 0.53934731, 0.04744624,\n",
       "         0.56690269, 0.09693404, 0.26045935, 0.00219879, 0.07982287]],\n",
       "\n",
       "       [[0.91694258, 0.96355041, 0.82085478, 0.08671529, 0.32901142,\n",
       "         0.3151091 , 0.25840229, 0.35347561, 0.51673311, 0.66232185],\n",
       "        [0.27537245, 0.93027783, 0.7268414 , 0.26244615, 0.15629505,\n",
       "         0.91964384, 0.08623289, 0.82943375, 0.07245723, 0.0747421 ],\n",
       "        [0.16450789, 0.26492091, 0.25576389, 0.33425773, 0.33609006,\n",
       "         0.45883156, 0.84020921, 0.19747133, 0.40436579, 0.19749114],\n",
       "        [0.51508228, 0.42608919, 0.1360035 , 0.03867004, 0.45527362,\n",
       "         0.94820812, 0.74114995, 0.67525483, 0.96350792, 0.8858881 ],\n",
       "        [0.67641316, 0.75951339, 0.41381221, 0.01190431, 0.40412328,\n",
       "         0.85099152, 0.68260942, 0.79981838, 0.06814183, 0.5175445 ],\n",
       "        [0.89932487, 0.39969661, 0.13056806, 0.84546468, 0.23247404,\n",
       "         0.34999763, 0.82281201, 0.78193167, 0.14887654, 0.08852798],\n",
       "        [0.21050821, 0.87822898, 0.43895353, 0.70291574, 0.46402087,\n",
       "         0.02757437, 0.49930557, 0.84112537, 0.68361692, 0.35373839],\n",
       "        [0.97580231, 0.28140456, 0.71436658, 0.491487  , 0.2005511 ,\n",
       "         0.15013053, 0.92241327, 0.19754068, 0.95830851, 0.67527455],\n",
       "        [0.80598449, 0.0600571 , 0.99754819, 0.74737025, 0.78854037,\n",
       "         0.75252153, 0.1826119 , 0.4497989 , 0.95633721, 0.06955621],\n",
       "        [0.85416004, 0.56069187, 0.4216603 , 0.12466859, 0.30352615,\n",
       "         0.1859945 , 0.14549062, 0.13525775, 0.90645545, 0.98468804]],\n",
       "\n",
       "       [[0.46236936, 0.73918592, 0.54963914, 0.64885071, 0.72074269,\n",
       "         0.35158127, 0.0663875 , 0.48565117, 0.64771158, 0.6021756 ],\n",
       "        [0.38512276, 0.33848579, 0.58424222, 0.80807288, 0.1232707 ,\n",
       "         0.54829511, 0.82702825, 0.28809561, 0.42796553, 0.53693578],\n",
       "        [0.37202108, 0.47724664, 0.79824211, 0.97034319, 0.39135066,\n",
       "         0.160689  , 0.46453062, 0.18705804, 0.74456508, 0.96541237],\n",
       "        [0.89687096, 0.56854996, 0.84299535, 0.64147559, 0.09474682,\n",
       "         0.67676337, 0.28796593, 0.0182683 , 0.81600551, 0.21757204],\n",
       "        [0.25507167, 0.46833361, 0.95997056, 0.64508158, 0.30318929,\n",
       "         0.54086542, 0.32394532, 0.70943011, 0.3528138 , 0.32438046],\n",
       "        [0.94384181, 0.09849676, 0.86813561, 0.83184985, 0.12761262,\n",
       "         0.00280748, 0.83871968, 0.44401626, 0.3099237 , 0.05623854],\n",
       "        [0.67270279, 0.53823094, 0.65913691, 0.28042949, 0.20840937,\n",
       "         0.4089917 , 0.48129931, 0.07162977, 0.5657365 , 0.51141542],\n",
       "        [0.77266265, 0.07860289, 0.03206921, 0.13059863, 0.02262101,\n",
       "         0.71681773, 0.61005722, 0.03911672, 0.76993961, 0.2896138 ],\n",
       "        [0.20404025, 0.32355763, 0.45841251, 0.04270206, 0.33339744,\n",
       "         0.52613372, 0.75081887, 0.52991973, 0.82500739, 0.62756954],\n",
       "        [0.64189168, 0.83345366, 0.47273942, 0.28135223, 0.88334179,\n",
       "         0.71831268, 0.26837013, 0.87406772, 0.21951474, 0.19762702]],\n",
       "\n",
       "       [[0.83874678, 0.75279265, 0.42848722, 0.52589887, 0.20206913,\n",
       "         0.73878049, 0.97451414, 0.83434475, 0.77918361, 0.54984218],\n",
       "        [0.68840207, 0.09328266, 0.47011308, 0.93647285, 0.70477734,\n",
       "         0.4415911 , 0.94755767, 0.99924668, 0.87939725, 0.80596406],\n",
       "        [0.25847665, 0.31197927, 0.02330868, 0.10435721, 0.77581117,\n",
       "         0.82308081, 0.97591021, 0.28867467, 0.65798403, 0.51522492],\n",
       "        [0.10704193, 0.02585395, 0.74504172, 0.98501451, 0.31311425,\n",
       "         0.76350698, 0.07151852, 0.04567954, 0.35501826, 0.37518872],\n",
       "        [0.30835291, 0.42526906, 0.51447272, 0.24363022, 0.25385324,\n",
       "         0.08511396, 0.08604229, 0.06813981, 0.7765147 , 0.64830833],\n",
       "        [0.2250943 , 0.23926803, 0.26813345, 0.49246581, 0.39681987,\n",
       "         0.88030147, 0.09756031, 0.80734252, 0.95778705, 0.97522062],\n",
       "        [0.43999195, 0.21748099, 0.93589229, 0.99041573, 0.12440968,\n",
       "         0.42349609, 0.36549921, 0.52453544, 0.38614241, 0.2516162 ],\n",
       "        [0.16331507, 0.57291315, 0.98877421, 0.91952806, 0.86456091,\n",
       "         0.84518058, 0.00600735, 0.06081456, 0.39391415, 0.89050763],\n",
       "        [0.28908479, 0.25269965, 0.37105999, 0.47194435, 0.95088549,\n",
       "         0.67488878, 0.74464128, 0.89985277, 0.12653414, 0.84304951],\n",
       "        [0.00179465, 0.03291101, 0.51368099, 0.57627099, 0.5961174 ,\n",
       "         0.95473216, 0.98107539, 0.03081091, 0.50753347, 0.02722983]],\n",
       "\n",
       "       [[0.40453596, 0.27511733, 0.46344977, 0.67136138, 0.33412349,\n",
       "         0.50555196, 0.30902916, 0.85182769, 0.19294416, 0.4123897 ],\n",
       "        [0.73432176, 0.42286131, 0.87134061, 0.32071725, 0.5919985 ,\n",
       "         0.43137616, 0.57542437, 0.28305436, 0.16732576, 0.73040878],\n",
       "        [0.61625551, 0.14215368, 0.36399748, 0.61199293, 0.24805144,\n",
       "         0.32045077, 0.64492128, 0.73302392, 0.09947708, 0.88133843],\n",
       "        [0.77024762, 0.56417093, 0.06471982, 0.4760776 , 0.16112651,\n",
       "         0.93481412, 0.58753547, 0.64441672, 0.03618896, 0.81785769],\n",
       "        [0.97757058, 0.74014093, 0.76159162, 0.45375075, 0.69550317,\n",
       "         0.45835283, 0.2679433 , 0.82008019, 0.91921948, 0.46611774],\n",
       "        [0.90795738, 0.9929662 , 0.43378585, 0.09550963, 0.39333861,\n",
       "         0.88381209, 0.61689617, 0.45948739, 0.41192764, 0.8296833 ],\n",
       "        [0.55704278, 0.20464003, 0.2710495 , 0.98542835, 0.82555379,\n",
       "         0.18989768, 0.61063644, 0.80510483, 0.28841465, 0.45599042],\n",
       "        [0.22877461, 0.81634395, 0.95189223, 0.48251275, 0.99171138,\n",
       "         0.10386869, 0.40879635, 0.82702208, 0.72251222, 0.77539089],\n",
       "        [0.72374241, 0.14391988, 0.92659156, 0.02962957, 0.06460968,\n",
       "         0.01184087, 0.76780907, 0.00262025, 0.51209435, 0.69797825],\n",
       "        [0.29787028, 0.17930765, 0.72349145, 0.16072575, 0.53726887,\n",
       "         0.45543703, 0.79600089, 0.44697038, 0.86065161, 0.88784186]],\n",
       "\n",
       "       [[0.30273796, 0.57809919, 0.19753334, 0.1772226 , 0.23190665,\n",
       "         0.67840378, 0.28149538, 0.07149422, 0.85058217, 0.84974301],\n",
       "        [0.12238007, 0.95266799, 0.6864311 , 0.43957509, 0.27273386,\n",
       "         0.72012939, 0.98005548, 0.63005512, 0.27143968, 0.3362572 ],\n",
       "        [0.45575475, 0.00688811, 0.17463125, 0.62561385, 0.85171851,\n",
       "         0.52141237, 0.12673706, 0.42525274, 0.59634951, 0.00123273],\n",
       "        [0.79347868, 0.12294635, 0.30391473, 0.1714563 , 0.48786059,\n",
       "         0.53847564, 0.39921893, 0.78354252, 0.8730495 , 0.98754881],\n",
       "        [0.19001101, 0.27534763, 0.40958561, 0.85494933, 0.42007766,\n",
       "         0.85974253, 0.82948022, 0.43449364, 0.84993307, 0.52266223],\n",
       "        [0.4985115 , 0.31039354, 0.9828438 , 0.1861375 , 0.34355345,\n",
       "         0.49295084, 0.56189271, 0.35405478, 0.34935316, 0.12156844],\n",
       "        [0.38598026, 0.88989428, 0.24383006, 0.25763856, 0.65383759,\n",
       "         0.96620025, 0.78789494, 0.48769243, 0.08839596, 0.27671963],\n",
       "        [0.55019865, 0.77811353, 0.08385062, 0.85447488, 0.19430013,\n",
       "         0.83569032, 0.45941557, 0.43940381, 0.64481852, 0.20257988],\n",
       "        [0.93127365, 0.14261704, 0.42123024, 0.34259848, 0.43058609,\n",
       "         0.65657579, 0.27100933, 0.45278741, 0.50316207, 0.60779035],\n",
       "        [0.258283  , 0.88744493, 0.66921631, 0.83996784, 0.85738728,\n",
       "         0.94056423, 0.51850847, 0.47675629, 0.09435163, 0.8604418 ]],\n",
       "\n",
       "       [[0.62300356, 0.01280621, 0.07730131, 0.34433584, 0.18109396,\n",
       "         0.0878893 , 0.18027346, 0.28523889, 0.73845131, 0.25974065],\n",
       "        [0.71813261, 0.34336067, 0.47651896, 0.01436326, 0.32491328,\n",
       "         0.36842292, 0.37909529, 0.0305442 , 0.68641462, 0.6890467 ],\n",
       "        [0.13604068, 0.92606762, 0.15417716, 0.27995914, 0.98680596,\n",
       "         0.06268606, 0.79480639, 0.36734073, 0.95844889, 0.3722939 ],\n",
       "        [0.99718895, 0.27792293, 0.43168994, 0.18284921, 0.0298537 ,\n",
       "         0.61148083, 0.47896612, 0.53451752, 0.22586523, 0.96570633],\n",
       "        [0.03013242, 0.46710554, 0.66948917, 0.86278061, 0.08840335,\n",
       "         0.7033453 , 0.27556031, 0.88120399, 0.08746152, 0.07560811],\n",
       "        [0.99675189, 0.29961384, 0.9632463 , 0.80329658, 0.45176814,\n",
       "         0.02341731, 0.81637989, 0.47458734, 0.86472483, 0.02115318],\n",
       "        [0.72282386, 0.74570462, 0.62685098, 0.50137469, 0.31485633,\n",
       "         0.88718568, 0.40241685, 0.17927807, 0.52594214, 0.32549128],\n",
       "        [0.88765864, 0.93733429, 0.00174393, 0.49992395, 0.2050853 ,\n",
       "         0.81872663, 0.82662184, 0.41104102, 0.16220065, 0.88461352],\n",
       "        [0.11902384, 0.16246225, 0.82895864, 0.79858097, 0.40110751,\n",
       "         0.21897447, 0.45946475, 0.28828263, 0.39463053, 0.74632214],\n",
       "        [0.58545575, 0.01187592, 0.86922612, 0.06558303, 0.01197011,\n",
       "         0.50803949, 0.64384451, 0.17787611, 0.91712768, 0.62750208]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposition of random tensor using rank 5\n",
    "factors = tldec.parafac(tensor, rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(weights, factors) : rank-5 CPTensor of shape (10, 10, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_tensor = tl.kruskal_to_tensor(factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tl.norm(tensor - reconstructed_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.07565625683765"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.624532328611688"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying various ranks and checking Frobenius norm\n",
    "\n",
    "factors = tldec.parafac(tensor, rank=3)\n",
    "reconstructed_tensor = tl.kruskal_to_tensor(factors)\n",
    "error = tl.norm(tensor - reconstructed_tensor)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.611501367850849"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying various ranks and checking Frobenius norm\n",
    "\n",
    "factors = tldec.parafac(tensor, rank=10)\n",
    "reconstructed_tensor = tl.kruskal_to_tensor(factors)\n",
    "error = tl.norm(tensor - reconstructed_tensor)\n",
    "error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.015378341940359199"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trying various ranks and checking Frobenius norm\n",
    "\n",
    "factors = tldec.parafac(tensor, rank=50)\n",
    "reconstructed_tensor = tl.kruskal_to_tensor(factors)\n",
    "error = tl.norm(tensor - reconstructed_tensor)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Tests On Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hpate061/anaconda3/envs/Research/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load roberta model\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.weight torch.Size([50265, 768])\n",
      "embeddings.position_embeddings.weight torch.Size([514, 768])\n",
      "embeddings.token_type_embeddings.weight torch.Size([1, 768])\n",
      "embeddings.LayerNorm.weight torch.Size([768])\n",
      "embeddings.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.0.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.0.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.0.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.0.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.0.output.dense.bias torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.0.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.1.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.1.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.1.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.1.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.1.output.dense.bias torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.1.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.2.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.2.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.2.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.2.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.2.output.dense.bias torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.2.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.3.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.3.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.3.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.3.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.3.output.dense.bias torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.3.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.4.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.4.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.4.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.4.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.4.output.dense.bias torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.4.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.5.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.5.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.5.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.5.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.5.output.dense.bias torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.5.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.6.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.6.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.6.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.6.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.6.output.dense.bias torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.6.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.7.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.7.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.7.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.7.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.7.output.dense.bias torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.7.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.8.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.8.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.8.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.8.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.8.output.dense.bias torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.8.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.9.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.9.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.9.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.9.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.9.output.dense.bias torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.9.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.10.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.10.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.10.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.10.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.10.output.dense.bias torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.10.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.query.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.query.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.key.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.key.bias torch.Size([768])\n",
      "encoder.layer.11.attention.self.value.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.self.value.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.dense.weight torch.Size([768, 768])\n",
      "encoder.layer.11.attention.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768])\n",
      "encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768])\n",
      "encoder.layer.11.intermediate.dense.bias torch.Size([3072])\n",
      "encoder.layer.11.output.dense.weight torch.Size([768, 3072])\n",
      "encoder.layer.11.output.dense.bias torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.weight torch.Size([768])\n",
      "encoder.layer.11.output.LayerNorm.bias torch.Size([768])\n",
      "pooler.dense.weight torch.Size([768, 768])\n",
      "pooler.dense.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "# print layers and their sizes\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selected_layers = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "selected_layers = [8, 9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_to_decompose = [] \n",
    "for layer in selected_layers:\n",
    "    #taking these 4 right now because they are the same size\n",
    "    weights_to_decompose.append(model.encoder.layer[layer].attention.self.query.weight.detach().numpy())\n",
    "    weights_to_decompose.append(model.encoder.layer[layer].attention.self.key.weight.detach().numpy())\n",
    "    weights_to_decompose.append(model.encoder.layer[layer].attention.self.value.weight.detach().numpy())\n",
    "    weights_to_decompose.append(model.encoder.layer[layer].attention.output.dense.weight.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = np.stack(weights_to_decompose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 768, 768)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = 5 # different ranks to try here \n",
    "factors = tldec.parafac(tensor, rank=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstructed_tensor_np = tl.kruskal_to_tensor(factors)\n",
    "reconstructed_tensor = torch.from_numpy(reconstructed_tensor_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(selected_layers):\n",
    "    index = i * 4\n",
    "    model.encoder.layer[layer].attention.self.query.weight = torch.nn.Parameter(reconstructed_tensor[index].clone().detach())\n",
    "    model.encoder.layer[layer].attention.self.key.weight = torch.nn.Parameter(reconstructed_tensor[index+1].clone().detach())\n",
    "    model.encoder.layer[layer].attention.self.value.weight = torch.nn.Parameter(reconstructed_tensor[index+2].clone().detach())\n",
    "    model.encoder.layer[layer].attention.output.dense.weight = torch.nn.Parameter(reconstructed_tensor[index+3].clone().detach())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
