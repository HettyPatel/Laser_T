
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Main: Msg: Starting intervention mode 1 on layer 0
/home/hpate061/anaconda3/envs/Research/lib/python3.11/site-packages/tensorly/backend/pytorch_backend.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(
Main: Msg: Edited and put modl on device in time 5 second
log_prob: <class 'torch.Tensor'> torch.Size([11, 50400])
question_answer_token_ids: <class 'torch.Tensor'> torch.Size([1, 11])
answer: <class 'str'> basketball
  0%|                                                                                                                                                                                                           | 0/18812 [00:00<?, ?it/s]The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.
  0%|                                                                                                                                                                                                           | 0/18812 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "/data/home/hpate061/Laser_T/src/TASER_intervention_gptj_bbh_qa.py", line 279, in <module>
    predictions = experiment.intervene(model=model,
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/hpate061/Laser_T/src/TASER_intervention_gptj_bbh_qa.py", line 111, in intervene
    log_prob_results = self.metrics.answer_log_prob(log_prob=log_prob,
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/hpate061/Laser_T/src/study_utils/metric_utils.py", line 268, in answer_log_prob
    answer_len = self.find_answer_len(question_answer_token_ids, answer, llm_tokenizer)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data/home/hpate061/Laser_T/src/study_utils/metric_utils.py", line 254, in find_answer_len
    pad = llm_tokenizer.decode(question_answer_token_ids[i:], clean_up_tokenization_spaces=False)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/hpate061/anaconda3/envs/Research/lib/python3.11/site-packages/transformers/tokenization_utils_base.py", line 3738, in decode
    return self._decode(
           ^^^^^^^^^^^^^
  File "/home/hpate061/anaconda3/envs/Research/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py", line 625, in _decode
    text = self._tokenizer.decode(token_ids, skip_special_tokens=skip_special_tokens)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: argument 'ids': 'list' object cannot be interpreted as an integer